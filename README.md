# ğŸ¤– Genestory Local Chatbot with RAG and LangGraph

A comprehensive local chatbot application built specifically for Genestory company information using Retrieval-Augmented Generation (RAG) and LangGraph workflow orchestration.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit     â”‚    â”‚   FastAPI       â”‚    â”‚   LangGraph     â”‚
â”‚   Frontend      â”‚â—„â”€â”€â–ºâ”‚   Backend       â”‚â—„â”€â”€â–ºâ”‚   Workflow      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Milvus      â”‚    â”‚    Models       â”‚    â”‚   Retrieval     â”‚
â”‚  Vector Store   â”‚â—„â”€â”€â–ºâ”‚  LLM + Embed    â”‚â—„â”€â”€â–ºâ”‚  Classification â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚   Reranking     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

### Core Functionality
- **ğŸ§  RAG-based Question Answering**: Retrieves relevant company information before generating responses
- **ğŸ”€ LangGraph Workflow**: Intelligent conversation flow with conditional logic
- **ğŸ¯ Query Classification**: Automatically determines if queries are company-related or general
- **ğŸ“Š Document Reranking**: Improves retrieval relevance with advanced reranking
- **ğŸ’¬ Multi-turn Conversations**: Maintains conversation history and context

### Technical Capabilities
- **ğŸ  Fully Local**: All models run locally (Gemma 3-4B, sentence-transformers)
- **âš¡ Vector Search**: Fast similarity search with Milvus database
- **ğŸ”„ Streaming Responses**: Real-time response generation
- **ğŸ“ˆ Comprehensive Monitoring**: Health checks, metrics, and system information
- **ğŸ”Œ RESTful API**: Full-featured API with OpenAPI documentation

### User Interface
- **ğŸ¨ Modern Web UI**: Clean, responsive Streamlit interface
- **ğŸ“ Document Management**: Upload and manage knowledge base documents
- **ğŸ“Š Response Analytics**: View classification details, retrieval stats, and processing time
- **âš™ï¸ System Controls**: Initialize models, check health, and manage the system

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Docker and Docker Compose
- 8GB+ RAM recommended
- CUDA-compatible GPU (optional, for faster inference)

### Installation

1. **Clone and Setup**
   ```bash
   git clone <repository-url>
   cd genestory-chatbot
   ./scripts/setup.sh
   ```

2. **Start the Application**
   ```bash
   ./scripts/start.sh
   ```

3. **Access the Application**
   - Web Interface: http://localhost:8501
   - API Documentation: http://localhost:8000/docs
   - Milvus UI: http://localhost:3000

4. **Stop the Application**
   ```bash
   ./scripts/stop.sh
   ```

## ğŸ“ Project Structure

```
genestory-chatbot/
â”œâ”€â”€ ğŸ“ backend/                 # FastAPI backend application
â”‚   â”œâ”€â”€ main.py                # API server and endpoints
â”‚   â””â”€â”€ models.py              # Pydantic models for API
â”œâ”€â”€ ğŸ“ frontend/                # Streamlit web interface
â”‚   â””â”€â”€ app.py                 # Main UI application
â”œâ”€â”€ ğŸ“ models/                  # Model management
â”‚   â”œâ”€â”€ llm/                   # Large Language Model (Gemma)
â”‚   â”‚   â””â”€â”€ manager.py         # LLM loading and inference
â”‚   â””â”€â”€ embedding/             # Embedding model (MiniLM)
â”‚       â””â”€â”€ manager.py         # Embedding generation
â”œâ”€â”€ ğŸ“ database/                # Vector database
â”‚   â””â”€â”€ milvus_manager.py      # Milvus operations
â”œâ”€â”€ ğŸ“ retrieval/               # Retrieval system
â”‚   â”œâ”€â”€ classification/        # Query classification
â”‚   â”‚   â”œâ”€â”€ classifier.py      # k-NN classifier
â”‚   â”‚   â””â”€â”€ sample_queries.py  # Training data
â”‚   â”œâ”€â”€ retriever.py           # Document retrieval
â”‚   â””â”€â”€ reranking/             # Document reranking
â”‚       â””â”€â”€ reranker.py        # Relevance reranking
â”œâ”€â”€ ğŸ“ langgraph/               # LangGraph workflow
â”‚   â””â”€â”€ workflow.py            # Conversation orchestration
â”œâ”€â”€ ğŸ“ knowledge_base/          # Knowledge base
â”‚   â”œâ”€â”€ documents/             # User uploaded documents
â”‚   â””â”€â”€ sample_data/           # Sample company information
â”œâ”€â”€ ğŸ“ config/                  # Configuration
â”‚   â””â”€â”€ settings.py            # Application settings
â”œâ”€â”€ ğŸ“ scripts/                 # Utility scripts
â”‚   â”œâ”€â”€ setup.sh               # Environment setup
â”‚   â”œâ”€â”€ start.sh               # Start application
â”‚   â”œâ”€â”€ stop.sh                # Stop application
â”‚   â””â”€â”€ load_documents.py      # Document loader
â”œâ”€â”€ docker-compose.yml         # Milvus database setup
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Configuration

### Environment Variables (.env)
```bash
# Application
APP_NAME=Genestory Chatbot
DEBUG=true

# Models
LLM_MODEL_NAME=google/gemma-2-2b-it
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

# Database
MILVUS_HOST=localhost
MILVUS_PORT=19530

# API
API_HOST=localhost
API_PORT=8000
STREAMLIT_PORT=8501
```

### Key Settings (config/settings.py)
- **Model paths and configurations**
- **Database connection settings**
- **Retrieval parameters (top-k, thresholds)**
- **Generation parameters (temperature, max tokens)**

## ğŸ§  How It Works

### 1. Query Processing Flow
```
User Query â†’ Classification â†’ Routing Decision
     â†“                â†“              â†“
Label 1: General â†’ Direct Response
Label 2: Company â†’ Retrieval â†’ Reranking â†’ Context-aware Response
```

### 2. Query Classification
- Uses sentence-transformers embeddings
- k-NN classification with cosine similarity
- Two labels: General/Greeting vs Company-related
- Confidence threshold for routing decisions

### 3. RAG Pipeline
1. **Encode Query**: Convert text to embedding vector
2. **Vector Search**: Find similar documents in Milvus
3. **Rerank Results**: Improve relevance ordering
4. **Generate Response**: LLM with retrieved context

### 4. LangGraph Workflow
- **Nodes**: Classification, Retrieval, Reranking, Generation
- **Conditional Edges**: Route based on query type
- **State Management**: Conversation history and metadata
- **Error Handling**: Graceful degradation

## ğŸ“Š API Endpoints

### Chat
- `POST /chat` - Process chat query
- `POST /chat/stream` - Stream chat response

### System Management
- `GET /health` - System health check
- `GET /system/info` - Comprehensive system information
- `POST /system/initialize` - Initialize models

### Document Management
- `POST /documents/upload` - Upload document
- `DELETE /documents/clear` - Clear all documents
- `GET /documents/stats` - Document statistics

### Model Management
- `POST /models/load` - Load specific models

## ğŸ” Usage Examples

### Chat API
```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What services does Genestory offer?",
    "conversation_history": []
  }'
```

### Document Upload
```bash
curl -X POST "http://localhost:8000/documents/upload" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "Genestory offers AI consulting services...",
    "source": "services.txt",
    "document_id": "doc_001"
  }'
```

## ğŸ› ï¸ Development

### Adding New Documents
1. Place documents in `knowledge_base/documents/`
2. Run `python scripts/load_documents.py`
3. Supported formats: `.md`, `.txt`

### Customizing Models
- Update model names in `config/settings.py`
- Models will be automatically downloaded from Hugging Face
- Ensure sufficient disk space and memory

### Extending Functionality
- Add new nodes to LangGraph workflow
- Implement custom retrieval strategies
- Add support for additional file formats

## ğŸ“‹ System Requirements

### Minimum Requirements
- **CPU**: 4 cores
- **RAM**: 8GB
- **Storage**: 20GB free space
- **Python**: 3.8+

### Recommended Requirements
- **CPU**: 8+ cores
- **RAM**: 16GB+
- **GPU**: CUDA-compatible (4GB+ VRAM)
- **Storage**: SSD with 50GB+ free space

## ğŸ”§ Troubleshooting

### Common Issues

**Model Loading Errors**
- Ensure sufficient RAM/VRAM
- Check internet connection for downloads
- Verify Hugging Face model availability

**Database Connection Issues**
- Ensure Docker is running
- Check Milvus container status: `docker ps`
- Restart containers: `docker-compose restart`

**Port Conflicts**
- Change ports in `.env` file
- Kill existing processes: `pkill -f uvicorn`

### Logs
- Backend: `logs/backend.log`
- Frontend: `logs/frontend.log`
- Document Loader: `logs/document_loader.log`

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **Hugging Face** - For model hosting and transformers library
- **Milvus** - For vector database capabilities
- **LangGraph** - For workflow orchestration
- **Streamlit** - For rapid UI development
- **FastAPI** - For robust API framework

## ğŸ“ Support

For support and questions:
- Create an issue in the repository
- Check the troubleshooting section
- Review the API documentation at `/docs`

---

**Built with â¤ï¸ for Genestory by the AI Team**
